---
title: "4GMæœ¬ï¼ˆApproaching (Almost) Any Machine Learning Problemï¼‰ã‚’èª­ã‚“ã æ„Ÿæƒ³ã‚’è©±ã—ã¾ã—ãŸï¼"
date: 2020-07-28T18:40:11+09:00
draft: false
---

Podcastã¯[ã“ã¡ã‚‰](https://anchor.fm/geek-engineer-future/)ã‹ã‚‰ğŸµ

ğŸ‘‰è³ªå•ã‚³ãƒ¡ãƒ³ãƒˆã¯[è³ªå•ç®±](https://peing.net/ja/04affd1e18a05d/message) or [Twitter](https://twitter.com/)ã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°[`#geek_engineer`](https://twitter.com/search?q=%23geek_engineer)ã«ã¦ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™ğŸ“®

## Summary

é€šç§°ã€Œ4GMæœ¬ã€ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹[Approaching (Almost) Any Machine Learning Problem](https://www.amazon.co.jp/Approaching-Almost-Machine-Learning-Problem/dp/8269211508)ã‚’èª­ã‚“ã ã®ã§ã€ãã®æ„Ÿæƒ³ã‚„å­¦ã³ãªã©ã‚’ãŠè©±ã—ã¾ã—ãŸï¼

## ã©ã‚“ãªæœ¬ï¼Ÿ

kaggleã§4ã¤ã®GMã®ç§°å·ã‚’æŒã¤[Abhishek Thakur æ°](https://www.kaggle.com/abhishek)ãŒæ›¸ã„ãŸæœ¬ã€‚
æ§‹æˆã¯ä»¥ä¸‹ã®é€šã‚Šï¼ˆç›®æ¬¡ï¼‰

```
ãƒ»Setting up your working environment
ãƒ»Supervised vs unsupervised learning
ãƒ»Cross-validation
ãƒ»Evaluation metrics
ãƒ»Arranging machine learning projects
ãƒ»Approaching categorical variables
ãƒ»Feature engineering
ãƒ»Feature Selection
ãƒ»Hyperparameter optimization
ãƒ»Approaching image classification & segmentation
ãƒ»Approaching text classification/regression
ãƒ»Approaching ensembling and stacking
ãƒ»Approaching reproducible code & model serving
```

ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚‚è±Šå¯Œã§ã€kaggleã§ã‚ˆãè©±é¡Œã«ãªã‚‹Cross-validationã‚„Feature engineeringã¨ã„ã£ãŸå†…å®¹ã‹ã‚‰ã€Approaching reproducible code & model servingã¨ã„ã£ãŸå®Ÿé‹ç”¨ã‚’è¦‹æ®ãˆãŸç« ã‚‚ã‚ã‚Šã€ã¨ã¦ã‚‚å‹‰å¼·ã«ãªã‚‹æœ¬ã ã£ãŸã€‚ï¼ˆDNNã¯PyTorchã«ã‚ˆã‚‹ã‚¹ãƒ‹ãƒšãƒƒãƒˆæœ‰ï¼‰

## ä»–ã®kaggleæ›¸ç±ã¨ã®æ£²ã¿åˆ†ã‘
å€‹äººçš„ã«ã¯ä¸‹è¨˜é †ç•ªã§å­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã¨æ€ã£ãŸã€‚

1. [Pythonã§ã¯ã˜ã‚ã‚‹Kaggleã‚¹ã‚¿ãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://www.amazon.co.jp/dp/4065190061)
2. [kaggleã§å‹ã¤ãƒ‡ãƒ¼ã‚¿åˆ†æã®æŠ€è¡“](https://www.amazon.co.jp/dp/4297108437)
3. [4GMæœ¬:Approaching (Almost) Any Machine Learning Problem](https://www.amazon.co.jp/dp/8269211508)

æœ¬æ›¸ç±ã¯ã€Œkaggleã§å‹ã¤ãƒ‡ãƒ¼ã‚¿åˆ†æã®æŠ€è¡“ã€ã¨å†…å®¹ãŒé‡è¤‡ã—ã¦ã„ã‚‹éƒ¨åˆ†ã‚‚å¤šã„ãŒã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç‚¹ã¨ã—ã¦ä¸‹è¨˜ãŒã‚ã‚‹ã¨æ„Ÿã˜ãŸã€‚
- ç‰¹å¾´é‡é¸æŠã®ç« ãŒã‚ã‚‹
- CVã‚„NLPã«ç‰¹åŒ–ã—ãŸç« ãŒã‚ã‚‹
- å®Ÿé‹ç”¨ã‚’è¦‹æ®ãˆãŸservingæ–¹æ³•ã«ã‚‚è§¦ã‚Œã¦ã„ã‚‹


## æ‰€æ„Ÿ
- çŸ¥ã‚‰ãªã„é–¢æ•°ï¼ˆe.g. numpy.ptpã‚„sklearnã®imputeï¼‰ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ããŸã€‚
- è‡ªåˆ†ã®çŸ¥è­˜ã®éš™é–“ã‚’åŸ‹ã‚ã¦ãã‚Œã‚‹ã‚ˆã†ãªå†…å®¹ã§ã‚ã£ãŸã€‚
- å½“ãŸã‚Šå‰ã ã‘ã©ã€åŸºæœ¬ã‚’å¿ å®Ÿã«ã“ãªã™ã®ãŒå¤§åˆ‡ãªã®ã ãªã€ã¨å®Ÿæ„Ÿã—ãŸã€‚


## åè¨€ã£ã½ã„ã‚‚ã®ã‚’æ®‹ã—ã¦ãŠã
- If you want to do feature engineering, split your data first. If you're going to build models, split your data first.
- Please note that itâ€™s usually better to create less and important features than to create hundreds of features in the first place.